{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vytvorim si ndarray a dataset kluce budu nazov adresara a kod speakra a hodnota bude nazov suboru \n",
    "# potom tomu priradim oznacenie train a testovacej sady tak aby kazdy bol aspon raz v testovacej \n",
    "# a zaroven sa nevyskytoval v treningovej \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "path=\"./train/audio/\"\n",
    "labels=os.listdir(path)\n",
    "file_list=[]\n",
    "user_id_list=[]\n",
    "word_list=[]\n",
    "for file in labels:\n",
    "    file_list.append(os.listdir(path+file))\n",
    "    user_id_list.append([x.split(sep=\"_\")[0] for x in file_list[-1]])\n",
    "    word_list.append([file for x in file_list[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=pd.DataFrame({'word':np.concatenate(word_list,axis=0),\n",
    "              'user_id':np.concatenate(user_id_list,axis=0),\n",
    "              'file':np.concatenate(file_list,axis=0)})\n",
    "# metadata.set_index([\"word\",\"user_id\"],inplace=True)\n",
    "pocty = metadata.groupby(['user_id']).count()\n",
    "pocty['fold']=0\n",
    "pocty.loc[0:360,'fold']=0\n",
    "pocty.loc[361:730,'fold']=1\n",
    "pocty.loc[731:1080,'fold']=2\n",
    "pocty.loc[1081:1420,'fold']=3\n",
    "pocty.loc[1421:,'fold']=4\n",
    "metadata = metadata.merge(pocty['fold'],on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "def prepare_dataset(path,metadata):\n",
    "    vectors = []\n",
    "    for i in metadata.index.tolist():\n",
    "        fldr = metadata.loc[i,'word']\n",
    "        file_name = metadata.loc[i,'file']\n",
    "        wave, sr = librosa.load(path+fldr+'/'+file_name, mono=True, sr=None)\n",
    "        # Downsampling\n",
    "        wave = wave[::3]\n",
    "        mfcc = librosa.feature.mfcc(wave, sr=16000)\n",
    "        vectors.append(mfcc)\n",
    "    return vectors\n",
    "\n",
    "m_data = metadata[metadata['word']!='_background_noise_']\n",
    "\n",
    "# vynecham speakrov iba s jednym slovom \n",
    "pocet_slov = m_data[['user_id','word']].groupby('user_id').nunique()\n",
    "zoznam = pocet_slov[pocet_slov['word']<2].index\n",
    "m_data = m_data[~m_data.user_id.isin(zoznam)]\n",
    "m_data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "data = prepare_dataset(path=path,metadata=m_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def sub_vec(vec,limit=4):\n",
    "    x = np.array(vec)\n",
    "    x = x.T\n",
    "    k = 0\n",
    "    if len(x)-limit>0:\n",
    "        k = random.randint(0,len(x)-limit)\n",
    "        x = x[k:k+limit]\n",
    "    return x\n",
    "    \n",
    "#chcem generovat batch data tak by som tam mal od rovnakeho pouzivatele rozne dve slova a nic ine.\n",
    "\n",
    "def gen_data(data,metadata,batch_size,fold=0,limit=4,train=True):\n",
    "    while True:\n",
    "        u_list={}\n",
    "\n",
    "        # robim to lokalne po batchoch \n",
    "        while len(u_list)<batch_size:\n",
    "            if train:\n",
    "                idx = random.choice(metadata[metadata['fold'] != fold].index.tolist())\n",
    "            else: \n",
    "                idx = random.choice(metadata[metadata['fold'] == fold].index.tolist())\n",
    "            \n",
    "            # vyberiem nahodne prvok a budujem zoznam\n",
    "            w,u = metadata.loc[idx,['word','user_id']]\n",
    "            \n",
    "            # ak sa tam nenachadza zaradim ho \n",
    "            if u_list.get(u,'xxx')=='xxx':\n",
    "                u_list[u] = w\n",
    "                k = 0\n",
    "\n",
    "                yield sub_vec(data[idx],limit=limit),0\n",
    "                \n",
    "                # najdem k nemu odlisny par ak ho najdem vyskocim z cyklu \n",
    "                while k<1:\n",
    "                    idy = random.choice(metadata[metadata['user_id'] == u].index.tolist())\n",
    "                    w2 = metadata.loc[idy,'word']\n",
    "                    if w2 != w:\n",
    "#                         print(idy)\n",
    "                        k += 1\n",
    "                        yield sub_vec(data[idy],limit=limit),0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "w,u=metadata.loc[124,['word','user_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diar_Model(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embeding = tf.keras.layers.Dense(embedding_dim,activation='relu')\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embeding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "data_train = tf.data.Dataset.from_generator(lambda: gen_data(data,m_data,batch_size,fold=0,train=True), \n",
    "                                            output_types=(tf.float32,tf.float32), \n",
    "                                            output_shapes=((4,20),())\n",
    "                                           )\n",
    "\n",
    "data_train = data_train.batch(batch_size)\n",
    "data_train.prefetch(4)\n",
    "\n",
    "data_test = tf.data.Dataset.from_generator(lambda: gen_data(data,m_data,batch_size,fold=0,train=False), \n",
    "                                            output_types=(tf.float32,tf.float32), \n",
    "                                            output_shapes=((4,20),())\n",
    "                                           )\n",
    "\n",
    "data_test = data_test.batch(batch_size)\n",
    "data_test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "model = Diar_Model(embedding_dim=32,rnn_units=128)\n",
    "for input_example_batch,true_example_batch in data_train.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from loss_metric import contrast_loss\n",
    "model.compile(optimizer='adam', loss=contrast_loss)\n",
    "steps=1000\n",
    "v_steps=100\n",
    "model.fit(data_train,epochs=5,validation_data = data_test,steps_per_epoch = steps,validation_steps = v_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_train,epochs=5,validation_data = data_test,steps_per_epoch = steps,validation_steps = v_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from loss_metric import contrast_loss\n",
    "import tensorflow as tf\n",
    "x = tf.random.uniform([32,6,20], minval=0, maxval=1)\n",
    "contrast_loss(x,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiarStep(tf.keras.Model):\n",
    "    def __init__(self, model, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def generate_one_step(self, inputs, states=[],pos=0,thc=0.9):\n",
    "        return_states=states.copy()\n",
    "        is_old = False\n",
    "\n",
    "        if len(states)>0:\n",
    "#             print(tf.shape(states))\n",
    "            st_old = states[pos]\n",
    "            predict_s, st = self.model(inputs=inputs, states=st_old,return_state=True)\n",
    "\n",
    "            st_old = st_old/tf.norm(st_old,axis=-1,keepdims=True)\n",
    "            pr = predict_s/tf.norm(predict_s,axis=-1,keepdims=True)\n",
    "            if tf.reduce_max(tf.tensordot(pr,st_old,axes=[0,0]))>thc:\n",
    "                return_pos = pos\n",
    "                return_states[pos] = st\n",
    "                is_old = True\n",
    "            \n",
    "            else:\n",
    "                for return_pos,st_old in enumerate(states):\n",
    "                    predict_s, st = self.model(inputs=inputs, states=st_old,return_state=True)\n",
    "                    pr = predict_s/tf.norm(predict_s,axis =-1,keepdims=True)\n",
    "                    st_old = st_old/tf.norm(st_old,axis =-1,keepdims=True)\n",
    "                    if tf.reduce_max(tf.tensordot(pr,st_old,axes=[0,0]))>thc:\n",
    "                        return_pos = pos\n",
    "                        return_states[pos] = st\n",
    "                        is_old = True\n",
    "                        break\n",
    "            if not is_old:\n",
    "                predict_s, st = self.model(inputs=inputs, states=None,return_state=True)\n",
    "                return_states.append(st)\n",
    "                return_pos = len(return_states)-1\n",
    "                \n",
    "\n",
    "        # este nemam ziadneho speakra teda prveho priradim do zoznamu stavov ... rozsirujem zoznam teda f je true\n",
    "        else:\n",
    "            predict_s, st = self.model(inputs=inputs, states=None,return_state=True)\n",
    "            return_states.append(st)\n",
    "            return_pos = len(return_states)-1\n",
    "\n",
    "        # vrat predikovanu reprezentaci, zoznam stavov, a poziciu\n",
    "        return predict_s, return_states, return_pos\n",
    "\n",
    "\n",
    "diar_step_model = DiarStep(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid = tf.data.Dataset.from_generator(lambda: gen_data(data,m_data,batch_size,fold=0,limit=1,train=False), \n",
    "                                            output_types=(tf.float32,tf.float32), \n",
    "                                            output_shapes=((1,20),())\n",
    "                                           )\n",
    "data_valid.take(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-ivory",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result=[]\n",
    "states=[]\n",
    "pos=0\n",
    "maxpos=0\n",
    "for s,pos_s in data_valid.take(64):\n",
    "    s=tf.expand_dims(s,axis=1)\n",
    "    rep, states,pos = diar_step_model.generate_one_step(s, states=states,pos=pos,thc=0.9)\n",
    "    result.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[[1,2,3,4],[1.1,2.1,3.1,4],[1.2,1.8,3,4]],[[2,1,4,3],[2.1,1.1,4.1,3],[2.2,0.8,4,3.1]]],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rep(inplen, channels, kernel_regularizer,initializer,constraint,batch_size=32,drp_rate=0.1,train=True):\n",
    "    # vstupna vrstva\n",
    "    input_layer = Input(shape=(inplen, channels),batch_size=batch_size, dtype = 'float32', name='input')\n",
    "    x = input_layer\n",
    "#     x = LayerNormalization(name = 'LN_0_'+str(i))(x)\n",
    "    y = x\n",
    "    x = Conv1D(filters=20, kernel_size=inplen-1, activation='relu', strides=1, \n",
    "                  kernel_constraint=constraint, \n",
    "                  kernel_initializer=initializer, \n",
    "                  kernel_regularizer=kernel_regularizer, padding='valid', trainable=train, name='conv_0_'+str(i))(x)\n",
    "    x = Conv1D(filters=20, kernel_size=1, activation='relu', strides=1, \n",
    "                  kernel_constraint=constraint, \n",
    "                  kernel_initializer=initializer, \n",
    "                  kernel_regularizer=kernel_regularizer, padding='valid', trainable=train, name='conv_0_'+str(i))(x)\n",
    "    x = GRU\n",
    "    x = Lambda(lambda tt: tf.reshape(tt,[-1,20]))(x)\n",
    "    x = LayerNormalization(axis=-1, center=False, scale=False)(x) \n",
    "    x = Lambda(lambda tt: tf.tensordot(tt,tt,axes=[1,1]))(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=gen_data(data,m_data,4)\n",
    "\n",
    "next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_PATH = \"./data/\"\n",
    "\n",
    "\n",
    "# Input: Folder Path\n",
    "# Output: Tuple (Label, Indices of the labels, one-hot encoded labels)\n",
    "def get_labels(path=DATA_PATH):\n",
    "    labels = os.listdir(path)\n",
    "    label_indices = np.arange(0, len(labels))\n",
    "    return labels, label_indices, to_categorical(label_indices)\n",
    "\n",
    "\n",
    "# Handy function to convert wav2mfcc\n",
    "def wav2mfcc(file_path, max_len=11):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    wave = wave[::3]\n",
    "    mfcc = librosa.feature.mfcc(wave, sr=16000)\n",
    "\n",
    "    # If maximum length exceeds mfcc lengths then pad the remaining ones\n",
    "    if (max_len > mfcc.shape[1]):\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Else cutoff the remaining parts\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    \n",
    "    return mfcc\n",
    "\n",
    "\n",
    "def save_data_to_array(path=DATA_PATH, max_len=11):\n",
    "    labels, _, _ = get_labels(path)\n",
    "\n",
    "    for label in labels:\n",
    "        # Init mfcc vectors\n",
    "        mfcc_vectors = []\n",
    "\n",
    "        wavfiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
    "        for wavfile in tqdm(wavfiles, \"Saving vectors of label - '{}'\".format(label)):\n",
    "            mfcc = wav2mfcc(wavfile, max_len=max_len)\n",
    "            mfcc_vectors.append(mfcc)\n",
    "        np.save(label + '.npy', mfcc_vectors)\n",
    "\n",
    "\n",
    "def get_train_test(split_ratio=0.6, random_state=42):\n",
    "    # Get available labels\n",
    "    labels, indices, _ = get_labels(DATA_PATH)\n",
    "\n",
    "    # Getting first arrays\n",
    "    X = np.load(labels[0] + '.npy')\n",
    "    y = np.zeros(X.shape[0])\n",
    "\n",
    "    # Append all of the dataset into one single array, same goes for y\n",
    "    for i, label in enumerate(labels[1:]):\n",
    "        x = np.load(label + '.npy')\n",
    "        X = np.vstack((X, x))\n",
    "        y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
    "\n",
    "    assert X.shape[0] == len(y)\n",
    "\n",
    "    return train_test_split(X, y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset(path=DATA_PATH,metadata):\n",
    "    vectors = []\n",
    "    for i in metadata.index.tolist():\n",
    "        fldr = metadata.loc[i,'word']\n",
    "        file_name = metadata.loc[i,'file']\n",
    "        wave, sr = librosa.load(path+wavfile+'/'+file_name, mono=True, sr=None)\n",
    "        # Downsampling\n",
    "        wave = wave[::3]\n",
    "        mfcc = librosa.feature.mfcc(wave, sr=16000)\n",
    "        vectors.append(mfcc)\n",
    "\n",
    "    metadata['mfcc'] = vectors\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def load_dataset(path=DATA_PATH):\n",
    "    data = prepare_dataset(path)\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for key in data:\n",
    "        for mfcc in data[key]['mfcc']:\n",
    "            dataset.append((key, mfcc))\n",
    "\n",
    "    return dataset[:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-testament",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-hotel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
